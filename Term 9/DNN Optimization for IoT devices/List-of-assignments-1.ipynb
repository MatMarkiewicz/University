{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "standard-block",
   "metadata": {},
   "source": [
    "# DNNOfIoT - List of assignments 1\n",
    "\n",
    "Mateusz Markiewicz - 298653"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-taste",
   "metadata": {},
   "source": [
    "## Is using linear activation function beneficial for multi-layer neural network?\n",
    "In short - no.\n",
    "\n",
    "The first layer of a network computes a linear combination of input signals. Linear transformation of linear combination is still a linear combination. Then the second layer computes a linear combination of the outputs of the first layer. But the linear combination of linear combinations gives a linear combination, therefore there are no benefits of having multi-layer architecture.\n",
    "\n",
    "Let's consider a network with 3 inputs - $I_1$, $I_2$, and $I_3$. Then we have the first layer with 2 hidden neurons - $H_1$ and $H_2$. Finally, we have a second layer with one neuron - $O_1$. For the sake of simplicity, let's consider activation function $f(x)=x$:\n",
    "\n",
    "\\begin{align}\n",
    "H_1 & = I_1 * w_{1,1} + I_2 * w_{1, 2} + I_3 * w_{1, 3} + b_{1} \\\\\n",
    "H_2 & = I_1 * w_{2,1} + I_2 * w_{2, 2} + I_3 * w_{2, 3} + b_{2} \\\\\n",
    "\\\\\n",
    "f(H_1) & = I_1 * w_{1,1} + I_2 * w_{1, 2} + I_3 * w_{1, 3} + b_{1} \\\\\n",
    "f(H_2) & = I_1 * w_{2,1} + I_2 * w_{2, 2} + I_3 * w_{2, 3} + b_{2} \\\\\n",
    "\\\\\n",
    "O_1 & = f(H_1) * w_{3, 1} + f(H_2) * w_{3, 2} + b_3 \\\\\n",
    "& = (I_1 * w_{1,1} + I_2 * w_{1, 2} + I_3 * w_{1, 3} + b_{1}) * w_{3, 1} \\\\\n",
    "& \\quad + (I_1 * w_{2,1} + I_2 * w_{2, 2} + I_3 * w_{2, 3} + b_{2}) * w_{3, 2} \\\\\n",
    "& \\quad + b_3 \\\\\n",
    "& = I_1 * (w_{1,1} * w_{3, 1} + w_{2,1} * w_{3, 2}) \\\\\n",
    "& \\quad + I_2 * (w_{1,2} * w_{3, 1} + w_{2,3} * w_{3, 2}) \\\\\n",
    "& \\quad + I_3 * (w_{1,3} * w_{3, 1} + w_{2,3} * w_{3, 2}) \\\\\n",
    "& \\quad + b_1 * w_{3, 1} + b_2 * w_{3, 2} +b_3 \\\\\n",
    "& = I_1 * w_{4,1} + I_2 * w_{4, 2} + I_3 * w_{4, 3} + b_{4}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-dictionary",
   "metadata": {},
   "source": [
    "## Derive a formula for updating a single weight $w_i$ for SGD optimizer and mean squared error loss\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-optimum",
   "metadata": {},
   "source": [
    "## Based on number of filters K, size of filters F, padding width P, stride S and input dimensions $W_1$ x $H_1$ x $C_1$ for convolutional layer, provide formulas for: number of weights per convolutional layer (and number of biases) and dimensions of output $W_2$ x $H_2$ x $C_2$\n",
    "\n",
    "Number of parameters: $F^2 * C_1 * K$ weights + $K$ biases\n",
    "\n",
    "Oputput size: \n",
    "\n",
    "$W_2 = \\frac{W_1 - F + 2P}{S} + 1$ \n",
    "\n",
    "$H_2 = \\frac{H_1 - F + 2P}{S} + 1$\n",
    "\n",
    "$C_2 = K$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
