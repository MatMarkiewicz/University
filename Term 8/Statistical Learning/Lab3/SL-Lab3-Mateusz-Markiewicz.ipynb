{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "worthy-nebraska",
   "metadata": {},
   "source": [
    "# Lab 3 - Mateusz Markiewicz (298653)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-counter",
   "metadata": {},
   "source": [
    "## 1\n",
    "Generate the design matrix $X_{500 \\times 450}$ such that its elements are iid random variables from $N(0, \\frac{1}{\\sqrt{N}})$. Then generate the vector of the response variable according to the model\n",
    "$Y = X \\beta + \\epsilon ,$ where $\\epsilon \\sim 2N(0,I)$, $\\beta_i = 10$ for $i \\in \\{1, \\cdots , k\\}$ and $\\beta_i = 0$ for $i \\in \\{k + 1, \\cdots , 450\\}$ and $k \\in \\{5, 20, 50\\}$.\n",
    "\n",
    "For 100 replications of the above experiments estimate the regression coefficients and/or identify important variables using:\n",
    "\n",
    "i) least squares\n",
    "    \n",
    "ii) ridge regression and LASSO with the tuning parameters selected by cross-validation\n",
    "    \n",
    "iii) use knockoffs with ridge and LASSO to identify important variables while keeping FDR equal to 0.2.\n",
    "    \n",
    "iv) adaptive LASSO I:\n",
    "\n",
    "first step: calculate weights using cross-validated LASSO (eliminate variables not selected by cross-validated LASSO)\n",
    "\n",
    "second step: use weighted cross-validated LASSO\n",
    "    \n",
    "v) adaptive LASSO II: \n",
    "\n",
    "first step: calculate $\\hat{\\beta}$ using cross-validated LASSO\n",
    "    \n",
    "second step: estimate $\\hat{\\sigma} = \\sqrt{\\frac{RSS}{n-k}}$, where RSS is from the cross-validated LASSO and k is the number of variables selected by cross-validated LASSO \n",
    "    \n",
    "third step: calculate weights $w_i = \\frac{\\hat{\\sigma}}{|\\hat{\\beta_i}|}$\n",
    "    \n",
    "fourth step: use weighted LASSO with the tuning parameter $\\lambda = \\hat{\\sigma} \\varPhi^{-1}(1-\\frac{\\alpha}{2p})$\n",
    "\n",
    "vi) adaptive SLOPE - as in point iv) but the in final stage use weighted SLOPE with BH sequence at FDR level 0.2.\n",
    "\n",
    "vii) extra 5 points - adaptive Bayesian SLOPE at FDR=0.2.\n",
    "\n",
    "a) For methods iii)-vii) estimate FDR and power.\n",
    "\n",
    "b) For all methods apart from iii) estimate the mean square errors of the estimators of $\\beta$ and $\\mu = X\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "velvet-equation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: Matrix\n",
      "\n",
      "Loaded glmnet 4.1-1\n",
      "\n",
      "Warning message:\n",
      "\"package 'SLOPE' was built under R version 4.0.5\"\n"
     ]
    }
   ],
   "source": [
    "library(glmnet);\n",
    "library(SLOPE);\n",
    "library(mvtnorm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dressed-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "n <- 500;\n",
    "p <- 450;\n",
    "k <- c(5, 20, 50);\n",
    "signal_strength<-10;\n",
    "l_k <- length(k);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "expanded-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "X <- matrix(rnorm(n*p,0,1/sqrt(n)),n,p);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "recreational-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas <- matrix(rep(0,l_k*p),p,l_k);\n",
    "for (i in 1:l_k){\n",
    "    betas[1:k[i],i]<-signal_strength;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "located-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb <- matrix(rep(0,l_k*n),n,l_k);\n",
    "for (i in 1:l_k){\n",
    "    Xb[,i]<-X%*%betas[,i];\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aware-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 <- matrix(rnorm(n*p,0,1/sqrt(n)),n,p);\n",
    "cX <- cbind(X, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "resistant-coating",
   "metadata": {},
   "outputs": [],
   "source": [
    "FDR <- function(betas_hat){\n",
    "    FDR_res <- rep(0,l_k);\n",
    "    for (i in 1:l_k){\n",
    "        all_discoveries <- apply(betas_hat[i, , ] != 0, c(1), sum);\n",
    "        all_discoveries <- pmax(all_discoveries,1);\n",
    "        false_discoveries <- apply(betas_hat[i, , (k[i]+1):p] != 0, c(1), sum)\n",
    "        FDR_res[i] <- mean(false_discoveries/all_discoveries);\n",
    "    }\n",
    "    return(FDR_res);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "supreme-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "power <- function(betas_hat){\n",
    "    power_res <- rep(0,l_k);\n",
    "    for (i in 1:l_k){\n",
    "        true_discoveries <- apply(betas_hat[i, , 1:k[i]] != 0, c(1), sum);\n",
    "        power_res[i] <- mean(true_discoveries/k[i]);\n",
    "    }\n",
    "    return(power_res);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "conventional-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_betas <- function(betas_hat){\n",
    "    mse_betas_res <- rep(0,l_k);\n",
    "    for (i in 1:l_k){\n",
    "        mse_betas_res[i] <-mean(apply(t(t(betas_hat[i,,]) - betas[,i])**2, c(1), sum));\n",
    "    }\n",
    "    return(mse_betas_res);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "infinite-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_mu <- function(betas_hat){\n",
    "    mse_mu_res <- rep(0,l_k);\n",
    "    for (i in 1:l_k){\n",
    "        mse_mu_res[i] <- mean(apply(t(X%*%(t(betas_hat[i,,]) - betas[,i]))**2, c(1), sum));\n",
    "    }\n",
    "    return(mse_mu_res);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sealed-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "Knockoff_betas <- function(w, beta_hat, q=0.2){\n",
    "    sorted <- sort(abs(w), decreasing=TRUE, index.return=TRUE);\n",
    "    fd<-cumsum(w[sorted$ix]<0); \n",
    "    nd<-cumsum(w[sorted$ix]>0);\n",
    "    fdr<-(fd+1)/nd;\n",
    "    betas_knockoff <- rep(0,p);\n",
    "    u1<-which(fdr<q);\n",
    "    if (length(u1)>0){\n",
    "        indopt<-max(u1);\n",
    "        a1<-sorted$ix[1:indopt];\n",
    "        a2<-which(w>0);\n",
    "        a3<-intersect(a1,a2);\n",
    "        betas_knockoff[a3]<-beta_hat[a3];\n",
    "    }\n",
    "    return(betas_knockoff);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wooden-notification",
   "metadata": {},
   "outputs": [],
   "source": [
    "reps <- 100;\n",
    "betas_ols <- array(rep(0,l_k*reps*p),c(l_k,reps,p));\n",
    "betas_ridge <- array(rep(0,l_k*reps*p),c(l_k,reps,p));\n",
    "betas_lasso <- array(rep(0,l_k*reps*p),c(l_k,reps,p));\n",
    "betas_kridge <- array(rep(0,l_k*reps*p),c(l_k,reps,p));\n",
    "betas_klasso <- array(rep(0,l_k*reps*p),c(l_k,reps,p));\n",
    "betas_adlasso <- array(rep(0,l_k*reps*p),c(l_k,reps,p));\n",
    "betas_adlasso2 <- array(rep(0,l_k*reps*p),c(l_k,reps,p));\n",
    "betas_adslope <- array(rep(0,l_k*reps*p),c(l_k,reps,p));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "binding-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (j in 1:reps){\n",
    "    epsilon <- 2*rnorm(n);\n",
    "    lambda_alasso <- qnorm(1-0.1/p);\n",
    "    for (i in 1:l_k){\n",
    "        Y <- Xb[,i] + epsilon;\n",
    "        \n",
    "        # ols\n",
    "        obj <- lm(Y~X-1);\n",
    "        betas_ols[i,j,] <- obj$coefficients;\n",
    "        \n",
    "        # ridge\n",
    "        obj2 <- cv.glmnet(X, Y, alpha=0, intercept=FALSE, standardize=FALSE);\n",
    "        betas_ridge_ <- coefficients(obj2, s='lambda.min')[2:(p+1),1];\n",
    "        betas_ridge[i,j,] <- betas_ridge_;\n",
    "        \n",
    "        # lasso\n",
    "        obj3 <- cv.glmnet(X, Y, alpha=1, intercept=FALSE, standardize=FALSE);\n",
    "        betas_lasso_ <- coefficients(obj3, s='lambda.min')[2:(p+1),1];\n",
    "        betas_lasso[i,j,] <- betas_lasso_;\n",
    "        lasso_indxs <- which(abs(betas_lasso_)>0);\n",
    "\n",
    "        # Knockoff Ridge\n",
    "        obj4 <- cv.glmnet(cX, Y, alpha=0, intercept=FALSE, standardize=FALSE);\n",
    "        betas_kridge_temp <- coefficients(obj4, s='lambda.min')[2:(2*p+1),1];\n",
    "        w_ridge <- abs(betas_kridge_temp[1:p])-abs(betas_kridge_temp[(p+1):(2*p)]);\n",
    "        betas_kridge[i,j,] <- Knockoff_betas(w_ridge, betas_ridge_);\n",
    "        \n",
    "        # Knockoff Lasso\n",
    "        obj5 <- cv.glmnet(cX, Y, alpha=1, intercept=FALSE, standardize=FALSE);\n",
    "        betas_klasso_temp <- coefficients(obj5, s='lambda.min')[2:(2*p+1),1];\n",
    "        w_lasso <- abs(betas_klasso_temp[1:p])-abs(betas_klasso_temp[(p+1):(2*p)]);\n",
    "        betas_klasso[i,j,] <- Knockoff_betas(w_lasso, betas_lasso_)\n",
    "        \n",
    "        # Adaptive Lasso I\n",
    "        X_ADLasso <- X[,lasso_indxs];\n",
    "        betas_adlasso_ <- betas_lasso_[lasso_indxs];\n",
    "        W_ADLasso_ <- betas_adlasso_;\n",
    "        X_ADLasso_temp <- sweep(X_ADLasso, 2, W_ADLasso_, '*');\n",
    "        obj6 <- cv.glmnet(X_ADLasso_temp, Y, alpha=1, intercept=FALSE, standardize=FALSE);\n",
    "        betas_adlasso_2 <- coefficients(obj6)[2:(length(lasso_indxs)+1)] * W_ADLasso_;\n",
    "        betas_adlasso_3 <- rep(0,p);\n",
    "        betas_adlasso_3[lasso_indxs] <- betas_adlasso_2;\n",
    "        betas_adlasso[i,j,] <- betas_adlasso_3;\n",
    "        \n",
    "        # Adaptive Lasso II\n",
    "        Lasso_RSS <- sum((Y- X%*%betas_lasso_)^2)\n",
    "        X_ADLasso2 <- X[,lasso_indxs];\n",
    "        betas_adlasso2_ <- betas_lasso_[lasso_indxs];\n",
    "        sigma_lassoCV <- sqrt(Lasso_RSS/(n-length(lasso_indxs)));\n",
    "        W_ADLasso_2<-abs(betas_adlasso2_)/sigma_lassoCV;\n",
    "        X_ADLasso2_temp <- sweep(X_ADLasso2, 2, W_ADLasso_2, '*');\n",
    "        obj7 <- glmnet(X_ADLasso2_temp, Y, intercept=FALSE, alpha=1, standardize=FALSE, lambda=sigma_lassoCV*lambda_alasso/n);\n",
    "        betas_adlasso2_2 <- coefficients(obj7)[2:(length(lasso_indxs)+1)] * W_ADLasso_2;\n",
    "        betas_adlasso2_3 <- rep(0,p);\n",
    "        betas_adlasso2_3[lasso_indxs] <- betas_adlasso2_2;\n",
    "        betas_adlasso2[i,j,] <- betas_adlasso2_3;\n",
    "        \n",
    "        # Adaptive SLOPE\n",
    "        W_ADSlope_ <- abs(betas_lasso_ + 1e-6)/sigma_lassoCV;\n",
    "        X_ADSlope_temp <- sweep(X, 2, W_ADSlope_, '*');\n",
    "        obj8 <- SLOPE(X_ADSlope_temp, Y, q=0.2, alpha=1/n*sigma_lassoCV, lambda='bh', solver='admm', max_passes=100, scale='none');\n",
    "        betas_adslope_ <- coefficients(obj8)[2:(p+1)] * W_ADSlope_;\n",
    "        betas_adslope[i,j,] <- betas_adslope_;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "noticed-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames1 <- c('K_Ridge','K_LASSO','ADLASSO','ADLASSO2','ADSLOPE')\n",
    "power_df = data.frame(\n",
    "    c1<-power(betas_kridge),\n",
    "    c2<-power(betas_klasso),\n",
    "    c3<-power(betas_adlasso),\n",
    "    c4<-power(betas_adlasso2),\n",
    "    c5<-power(betas_adslope)\n",
    ");\n",
    "colnames(power_df) <- colnames1;\n",
    "rownames(power_df) <- k;\n",
    "\n",
    "fdr_df = data.frame(\n",
    "    c1<-FDR(betas_kridge),\n",
    "    c2<-FDR(betas_klasso),\n",
    "    c3<-FDR(betas_adlasso),\n",
    "    c4<-FDR(betas_adlasso2),\n",
    "    c5<-FDR(betas_adslope)\n",
    ");\n",
    "colnames(fdr_df) <- colnames1;\n",
    "rownames(fdr_df) <- k;\n",
    "\n",
    "\n",
    "colnames2 <- c('OLS', \"Ridge\", \"LASSO\",'ADLASSO','ADLASSO2','ADSLOPE')\n",
    "betas_mse_df = data.frame(\n",
    "    c1<-mse_betas(betas_ols),\n",
    "    c2<-mse_betas(betas_ridge),\n",
    "    c3<-mse_betas(betas_lasso),\n",
    "    c4<-mse_betas(betas_adlasso),\n",
    "    c5<-mse_betas(betas_adlasso2),\n",
    "    c6<-mse_betas(betas_adslope)\n",
    ");\n",
    "colnames(betas_mse_df) <- colnames2;\n",
    "rownames(betas_mse_df) <- k;\n",
    "\n",
    "mu_mse_df = data.frame(\n",
    "    c1<-mse_mu(betas_ols),\n",
    "    c2<-mse_mu(betas_ridge),\n",
    "    c3<-mse_mu(betas_lasso),\n",
    "    c4<-mse_mu(betas_adlasso),\n",
    "    c5<-mse_mu(betas_adlasso2),\n",
    "    c6<-mse_mu(betas_adslope)\n",
    ");\n",
    "colnames(mu_mse_df) <- colnames2;\n",
    "rownames(mu_mse_df) <- k;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "trying-primary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Power:\"\n",
      "   K_Ridge K_LASSO ADLASSO ADLASSO2 ADSLOPE\n",
      "5   0.3620  0.4640  0.9940   0.9640  0.9680\n",
      "20  0.5005  0.9375  0.9970   0.9600  0.9745\n",
      "50  0.5382  0.9472  0.9958   0.9664  0.9850\n"
     ]
    }
   ],
   "source": [
    "print('Power:')\n",
    "print(power_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "approximate-favor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"FDR:\"\n",
      "     K_Ridge   K_LASSO   ADLASSO  ADLASSO2   ADSLOPE\n",
      "5  0.1544815 0.1425676 0.6243929 0.1801726 0.2385251\n",
      "20 0.1293548 0.1687399 0.6531812 0.1611591 0.2685747\n",
      "50 0.1922487 0.2006646 0.5083831 0.1376820 0.2570011\n"
     ]
    }
   ],
   "source": [
    "print('FDR:')\n",
    "print(fdr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "opening-turner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Betas MSE:\"\n",
      "        OLS     Ridge    LASSO  ADLASSO  ADLASSO2   ADSLOPE\n",
      "5  17552.88  416.2828 139.3958 206.3660  94.44338  89.19459\n",
      "20 17552.88 1211.9450 455.2724 515.2532 373.04804 327.49461\n",
      "50 17552.88 2096.0132 923.6777 834.6129 887.33294 729.44346\n"
     ]
    }
   ],
   "source": [
    "print('Betas MSE:')\n",
    "print(betas_mse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ahead-acrylic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Mu MSE:\"\n",
      "        OLS     Ridge    LASSO  ADLASSO  ADLASSO2   ADSLOPE\n",
      "5  1809.535  358.3019 129.9146 202.6428  89.72967  84.52688\n",
      "20 1809.535  765.4374 365.9268 449.4459 320.89496 279.11631\n",
      "50 1809.535 1059.5171 643.0905 616.8512 683.93059 549.11037\n"
     ]
    }
   ],
   "source": [
    "print('Mu MSE:')\n",
    "print(mu_mse_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-prison",
   "metadata": {},
   "source": [
    "### Power:\n",
    "\n",
    "Ridge Regression with knockoffs has small powers, especially when k=5 (~0.35). For larger k values the power increase to a value of ~0.5, in some of my experiments power for k=20 was equal to 0.7, but it's still not a good result.\n",
    "\n",
    "Lasso Regression with knockoffs has small power for k=5, but it was expected. For larger k values power increase to ~0.95. Therefore we can see that the knockoff method doesn't work when k is small.\n",
    "    \n",
    "Both versions of the Adaptive Lasso have good and stable power for each k value, however, we can see a bit better performers (in terms of power) of the first version.\n",
    "\n",
    "The power of the adaptive SLOPE is high (but a bit smaller than the power of adaptive Lasso) and it increases with k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-application",
   "metadata": {},
   "source": [
    "### False discoveries rate:\n",
    "\n",
    "We can see that both Ridge and Lasso regressions with knockoffs are able to control FDR at a given level (0.2). For smaller k values their FDRs are even smaller.\n",
    "    \n",
    "The second version of the adaptive Lasso is doing much better, as expected.\n",
    "    \n",
    "FDR of the adaptive SLOPE is a bit higher than the given level (0.2), but it's still acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-viking",
   "metadata": {},
   "source": [
    "### Mean squared error:\n",
    "\n",
    "The standard version of Lasso has much smaller MSEs than the standard version of Ridge, as expected. \n",
    "    \n",
    "The second version of adaptive Lasso gives smaller MSE, but we could predict that using the FDRs.The first version gives worse MSE than the standard Lasso.\n",
    "    \n",
    "The best in terms of MSEs is the adaptive version of SLOPE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-raleigh",
   "metadata": {},
   "source": [
    "The first version of adaptive Lasso was capable of selecting nearly all true variables, but it also selects many false discoveries, therefore we have high MSE. Both the second version of adaptive Lasso and adaptive SLOPE perform really well. SLOPE has a bit higher power, but Lasso has a smaller FDR. When k is high we should also consider Lasso with knockoffs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-defeat",
   "metadata": {},
   "source": [
    "## 2\n",
    "Repeat Problem 1 when $X_i \\sim N(0,\\frac{1}{n}\\Sigma)$, where $\\Sigma_{ii} = 1$ and $\\Sigma_{ij} = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "suffering-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "n <- 500;\n",
    "p <- 450;\n",
    "k <- c(5, 20, 50);\n",
    "signal_strength<-10;\n",
    "l_k <- length(k);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "environmental-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma <- matrix(0.5, p, p) \n",
    "diag(sigma) <- 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "inner-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "X <- (rmvnorm(n,numeric(p),sigma))/sqrt(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "public-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "s<-min(eigen(sigma)$values);\n",
    "s<-min(2*s,1);\n",
    "sseq=c(rep(s,p));\n",
    "V=2*diag(sseq)-diag(sseq)%*%solve(sigma)%*%diag(sseq);\n",
    "mu<-X-X%*%solve(sigma)%*%diag(sseq);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "physical-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xn<-mu+rmvnorm(n,rep(0,p),V)/sqrt(n);\n",
    "cX<-cbind(X,Xn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "allied-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas <- matrix(rep(0,l_k*p),p,l_k);\n",
    "for (i in 1:l_k){\n",
    "    betas[1:k[i],i]<-signal_strength;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eastern-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb <- matrix(rep(0,l_k*n),n,l_k);\n",
    "for (i in 1:l_k){\n",
    "    Xb[,i]<-X%*%betas[,i];\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "graphic-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "reps <- 100;\n",
    "betas_ols <- array(rep(0,l_k*reps*p),c(l_k,reps,p));\n",
    "betas_ridge <- array(rep(0,l_k*reps*p),c(l_k,reps,p));\n",
    "betas_lasso <- array(rep(0,l_k*reps*p),c(l_k,reps,p));\n",
    "betas_kridge <- array(rep(0,l_k*reps*p),c(l_k,reps,p));\n",
    "betas_klasso <- array(rep(0,l_k*reps*p),c(l_k,reps,p));\n",
    "betas_adlasso <- array(rep(0,l_k*reps*p),c(l_k,reps,p));\n",
    "betas_adlasso2 <- array(rep(0,l_k*reps*p),c(l_k,reps,p));\n",
    "betas_adslope <- array(rep(0,l_k*reps*p),c(l_k,reps,p));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "following-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (j in 1:reps){\n",
    "    epsilon <- 2*rnorm(n);\n",
    "    lambda_alasso <- qnorm(1-0.1/p);\n",
    "    for (i in 1:l_k){\n",
    "        Y <- Xb[,i] + epsilon;\n",
    "        \n",
    "        # ols\n",
    "        obj <- lm(Y~X-1);\n",
    "        betas_ols[i,j,] <- obj$coefficients;\n",
    "        \n",
    "        # ridge\n",
    "        obj2 <- cv.glmnet(X, Y, alpha=0, intercept=FALSE, standardize=FALSE);\n",
    "        betas_ridge_ <- coefficients(obj2, s='lambda.min')[2:(p+1),1];\n",
    "        betas_ridge[i,j,] <- betas_ridge_;\n",
    "        \n",
    "        # lasso\n",
    "        obj3 <- cv.glmnet(X, Y, alpha=1, intercept=FALSE, standardize=FALSE);\n",
    "        betas_lasso_ <- coefficients(obj3, s='lambda.min')[2:(p+1),1];\n",
    "        betas_lasso[i,j,] <- betas_lasso_;\n",
    "        lasso_indxs <- which(abs(betas_lasso_)>0);\n",
    "\n",
    "        # Knockoff Ridge\n",
    "        obj4 <- cv.glmnet(cX, Y, alpha=0, intercept=FALSE, standardize=FALSE);\n",
    "        betas_kridge_temp <- coefficients(obj4, s='lambda.min')[2:(2*p+1),1];\n",
    "        w_ridge <- abs(betas_kridge_temp[1:p])-abs(betas_kridge_temp[(p+1):(2*p)]);\n",
    "        betas_kridge[i,j,] <- Knockoff_betas(w_ridge, betas_ridge_);\n",
    "        \n",
    "        # Knockoff Lasso\n",
    "        obj5 <- cv.glmnet(cX, Y, alpha=1, intercept=FALSE, standardize=FALSE);\n",
    "        betas_klasso_temp <- coefficients(obj5, s='lambda.min')[2:(2*p+1),1];\n",
    "        w_lasso <- abs(betas_klasso_temp[1:p])-abs(betas_klasso_temp[(p+1):(2*p)]);\n",
    "        betas_klasso[i,j,] <- Knockoff_betas(w_lasso, betas_lasso_)\n",
    "        \n",
    "        if (length(lasso_indxs)<2){\n",
    "            next;\n",
    "        }\n",
    "        \n",
    "        # Adaptive Lasso I\n",
    "        X_ADLasso <- X[,lasso_indxs];\n",
    "        betas_adlasso_ <- betas_lasso_[lasso_indxs];\n",
    "        W_ADLasso_ <- betas_adlasso_;\n",
    "        X_ADLasso_temp <- sweep(X_ADLasso, 2, W_ADLasso_, '*');\n",
    "        obj6 <- cv.glmnet(X_ADLasso_temp, Y, alpha=1, intercept=FALSE, standardize=FALSE);\n",
    "        betas_adlasso_2 <- coefficients(obj6)[2:(length(lasso_indxs)+1)] * W_ADLasso_;\n",
    "        betas_adlasso_3 <- rep(0,p);\n",
    "        betas_adlasso_3[lasso_indxs] <- betas_adlasso_2;\n",
    "        betas_adlasso[i,j,] <- betas_adlasso_3;\n",
    "        \n",
    "        # Adaptive Lasso II\n",
    "        Lasso_RSS <- sum((Y- X%*%betas_lasso_)^2)\n",
    "        X_ADLasso2 <- X[,lasso_indxs];\n",
    "        betas_adlasso2_ <- betas_lasso_[lasso_indxs];\n",
    "        sigma_lassoCV <- sqrt(Lasso_RSS/(n-length(lasso_indxs)));\n",
    "        W_ADLasso_2<-abs(betas_adlasso2_)/sigma_lassoCV;\n",
    "        X_ADLasso2_temp <- sweep(X_ADLasso2, 2, W_ADLasso_2, '*');\n",
    "        obj7 <- glmnet(X_ADLasso2_temp, Y, intercept=FALSE, alpha=1, standardize=FALSE, lambda=sigma_lassoCV*lambda_alasso/n);\n",
    "        betas_adlasso2_2 <- coefficients(obj7)[2:(length(lasso_indxs)+1)] * W_ADLasso_2;\n",
    "        betas_adlasso2_3 <- rep(0,p);\n",
    "        betas_adlasso2_3[lasso_indxs] <- betas_adlasso2_2;\n",
    "        betas_adlasso2[i,j,] <- betas_adlasso2_3;\n",
    "        \n",
    "        # Adaptive SLOPE\n",
    "        W_ADSlope_ <- abs(betas_lasso_ + 1e-6)/sigma_lassoCV;\n",
    "        X_ADSlope_temp <- sweep(X, 2, W_ADSlope_, '*');\n",
    "        obj8 <- SLOPE(X_ADSlope_temp, Y, q=0.2, alpha=1/n*sigma_lassoCV, lambda='bh', solver='admm', max_passes=100, scale='none');\n",
    "        betas_adslope_ <- coefficients(obj8)[2:(p+1)] * W_ADSlope_;\n",
    "        betas_adslope[i,j,] <- betas_adslope_;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "later-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames1 <- c('K_Ridge','K_LASSO','ADLASSO','ADLASSO2','ADSLOPE')\n",
    "power_df = data.frame(\n",
    "    c1<-power(betas_kridge),\n",
    "    c2<-power(betas_klasso),\n",
    "    c3<-power(betas_adlasso),\n",
    "    c4<-power(betas_adlasso2),\n",
    "    c5<-power(betas_adslope)\n",
    ");\n",
    "colnames(power_df) <- colnames1;\n",
    "rownames(power_df) <- k;\n",
    "\n",
    "fdr_df = data.frame(\n",
    "    c1<-FDR(betas_kridge),\n",
    "    c2<-FDR(betas_klasso),\n",
    "    c3<-FDR(betas_adlasso),\n",
    "    c4<-FDR(betas_adlasso2),\n",
    "    c5<-FDR(betas_adslope)\n",
    ");\n",
    "colnames(fdr_df) <- colnames1;\n",
    "rownames(fdr_df) <- k;\n",
    "\n",
    "\n",
    "colnames2 <- c('OLS', \"Ridge\", \"LASSO\",'ADLASSO','ADLASSO2','ADSLOPE')\n",
    "betas_mse_df = data.frame(\n",
    "    c1<-mse_betas(betas_ols),\n",
    "    c2<-mse_betas(betas_ridge),\n",
    "    c3<-mse_betas(betas_lasso),\n",
    "    c4<-mse_betas(betas_adlasso),\n",
    "    c5<-mse_betas(betas_adlasso2),\n",
    "    c6<-mse_betas(betas_adslope)\n",
    ");\n",
    "colnames(betas_mse_df) <- colnames2;\n",
    "rownames(betas_mse_df) <- k;\n",
    "\n",
    "mu_mse_df = data.frame(\n",
    "    c1<-mse_mu(betas_ols),\n",
    "    c2<-mse_mu(betas_ridge),\n",
    "    c3<-mse_mu(betas_lasso),\n",
    "    c4<-mse_mu(betas_adlasso),\n",
    "    c5<-mse_mu(betas_adlasso2),\n",
    "    c6<-mse_mu(betas_adslope)\n",
    ");\n",
    "colnames(mu_mse_df) <- colnames2;\n",
    "rownames(mu_mse_df) <- k;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "figured-church",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Power:\"\n",
      "   K_Ridge K_LASSO ADLASSO ADLASSO2 ADSLOPE\n",
      "5   0.2240  0.2260   0.730   0.7600  0.7840\n",
      "20  0.5755  0.8155   0.891   0.9040  0.9250\n",
      "50  0.2760  0.8790   0.911   0.9094  0.9412\n"
     ]
    }
   ],
   "source": [
    "print('Power:')\n",
    "print(power_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "random-technician",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"FDR:\"\n",
      "     K_Ridge   K_LASSO   ADLASSO  ADLASSO2   ADSLOPE\n",
      "5  0.1260566 0.1480514 0.3226147 0.4082288 0.4537907\n",
      "20 0.2866828 0.1925669 0.2907617 0.2716989 0.3430597\n",
      "50 0.2904784 0.1850489 0.2161787 0.2008199 0.2741512\n"
     ]
    }
   ],
   "source": [
    "print('FDR:')\n",
    "print(fdr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "superior-glance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Betas MSE:\"\n",
      "       OLS     Ridge     LASSO   ADLASSO  ADLASSO2   ADSLOPE\n",
      "5  36672.3  454.3644  243.8364  313.8162  241.2622  235.3232\n",
      "20 36672.3 1358.0324  589.9292  722.0914  617.6693  569.1529\n",
      "50 36672.3 3039.1407 1183.0892 1559.9269 1520.9439 1296.9548\n"
     ]
    }
   ],
   "source": [
    "print('Betas MSE:')\n",
    "print(betas_mse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "virtual-montgomery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Mu MSE:\"\n",
      "        OLS     Ridge    LASSO  ADLASSO ADLASSO2  ADSLOPE\n",
      "5  1792.552  203.6159 116.7181 193.0652 121.1584 116.1730\n",
      "20 1792.552  595.7304 277.9815 349.0570 292.7120 266.7723\n",
      "50 1792.552 1293.7556 489.7624 623.3602 607.0306 523.7840\n"
     ]
    }
   ],
   "source": [
    "print('Mu MSE:')\n",
    "print(mu_mse_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-microphone",
   "metadata": {},
   "source": [
    "### Power:\n",
    "    \n",
    "We can observe a decrease in powers for all methods, especially for k=5. The first version of adaptive Lasso no longer leads the ranking, now the adaptive version of SLOPE performs the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-decimal",
   "metadata": {},
   "source": [
    "### False discoveries rate:\n",
    "\n",
    "We can also observe an increase in FDRs for all methods apart from Lasso with knockoffs and the first version of adaptive Lasso. Now the first version of adaptive Lasso performs similarly (or sometimes even better) than the second one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-screen",
   "metadata": {},
   "source": [
    "### Mean squared error:\n",
    "\n",
    "For k=5 and k=20 we're still getting the smallest MSEs for the adaptive SLOPE. For larger k value standard version of Lasso outperforms the others. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-inventory",
   "metadata": {},
   "source": [
    "We can see that when there is a correlation between variables it's much harder to discover true variables. The second version of adaptive Lasso and the adaptive SLOPE still performs well, but it may happen that standard Lasso gives smaller MSE than them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
