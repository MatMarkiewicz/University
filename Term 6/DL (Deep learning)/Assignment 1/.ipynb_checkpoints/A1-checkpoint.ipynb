{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "## Important notes\n",
    "**Submission deadline:**\n",
    "* **Thursday, 12.03.2020**\n",
    "\n",
    "**Points: 13 + 2bp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard IPython notebook imports\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "from io import StringIO\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import scipy.stats as sstats\n",
    "import scipy.optimize as sopt\n",
    "from scipy.linalg import solve_triangular\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is meant to test your skills in course pre-requisites:  Scientific Python programming and  Machine Learning. If it is hard, I strongly advise you to drop the course.\n",
    "\n",
    "Please use GitHub’s [pull requests](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests) and issues to send corrections!\n",
    "\n",
    "You can solve the assignment in any system you like, but we encourage you to try out [Google Colab](https://colab.research.google.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment text\n",
    "1. **[1p]** Download data competition from a Kaggle competition on sentiment prediction from [[https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data)].  Keep only full sentences, i.e. for each `SenteceId` keep only the entry with the lowest `PhraseId`.  Use first 7000 sentences as a `train set` and the remaining 1529 sentences as the `test set`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.read_table('train.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF2 = DF[[True] + [DF['SentenceId'][i] != DF['SentenceId'][i-1] for i in range(1,156060)]]\n",
    "DF = DF.groupby('SentenceId',as_index = False).first()\n",
    "DF = DF.drop(['SentenceId','PhraseId'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>This quiet , introspective and entertaining in...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Even fans of Ismail Merchant 's work , I suspe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A positively thrilling combination of ethnogra...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>A comedy-drama of nearly epic proportions root...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Narratively , Trouble Every Day is a plodding ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>The Importance of Being Earnest , so thick wit...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>But it does n't leave you with much .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>You could hate it for the same reason .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Phrase  Sentiment\n",
       "0  A series of escapades demonstrating the adage ...          1\n",
       "1  This quiet , introspective and entertaining in...          4\n",
       "2  Even fans of Ismail Merchant 's work , I suspe...          1\n",
       "3  A positively thrilling combination of ethnogra...          3\n",
       "4  Aggressive self-glorification and a manipulati...          1\n",
       "5  A comedy-drama of nearly epic proportions root...          4\n",
       "6  Narratively , Trouble Every Day is a plodding ...          1\n",
       "7  The Importance of Being Earnest , so thick wit...          3\n",
       "8              But it does n't leave you with much .          1\n",
       "9            You could hate it for the same reason .          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8529, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF['Sentiment'] = DF['Sentiment']/4\n",
    "DF['Phrase'] = list(map(lambda s: re.sub(r'[^\\w\\s]','',s.lower()),DF['Phrase']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df = train_test_split(DF,train_size=7000,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>a series of escapades demonstrating the adage ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>this quiet  introspective and entertaining ind...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>even fans of ismail merchant s work  i suspect...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>a positively thrilling combination of ethnogra...</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>aggressive selfglorification and a manipulativ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6995</td>\n",
       "      <td>snoots will no doubt rally to its cause  trott...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6996</td>\n",
       "      <td>it s better suited for the history or biograph...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6997</td>\n",
       "      <td>buries an interesting storyline</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6998</td>\n",
       "      <td>this one is a few bits funnier than malle s du...</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6999</td>\n",
       "      <td>the film has several strong performances</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Phrase  Sentiment\n",
       "0     a series of escapades demonstrating the adage ...       0.25\n",
       "1     this quiet  introspective and entertaining ind...       1.00\n",
       "2     even fans of ismail merchant s work  i suspect...       0.25\n",
       "3     a positively thrilling combination of ethnogra...       0.75\n",
       "4     aggressive selfglorification and a manipulativ...       0.25\n",
       "...                                                 ...        ...\n",
       "6995  snoots will no doubt rally to its cause  trott...       0.25\n",
       "6996  it s better suited for the history or biograph...       0.25\n",
       "6997                    buries an interesting storyline       0.50\n",
       "6998  this one is a few bits funnier than malle s du...       0.75\n",
       "6999          the film has several strong performances        0.75\n",
       "\n",
       "[7000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **[1p]** Prepare the data for logistic regression:\n",
    "\tMap the sentiment scores $0,1,2,3,4$ to a probability of the sentence being by setting $p(\\textrm{positive}) = \\textrm{sentiment}/4$.\n",
    "\tBuild a dictionary of at most 20000 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = defaultdict(int)\n",
    "for i in train_df.index:\n",
    "    sample = train_df.loc[i]\n",
    "    for word in sample['Phrase'].split():\n",
    "        WORDS[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14743"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2000\n",
    "Common_words_list = list(sorted(list(WORDS.keys()),key = WORDS.get,reverse=True))[:K]\n",
    "Common_words = {word:i for i,word in enumerate(Common_words_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **[3p]** Treat each document as a bag of words. e.g. if the vocabulary is \n",
    "\t```\n",
    "\t0: the\n",
    "\t1: good\n",
    "\t2: movie\n",
    "\t3: is\n",
    "\t4: not\n",
    "\t5: a\n",
    "\t6: funny\n",
    "\t```\n",
    "\tThen the encodings can be:\n",
    "\t```\n",
    "\tgood:                           [0,1,0,0,0,0,0]\n",
    "\tnot good:                       [0,1,0,0,1,0,0] \n",
    "\tthe movie is not a funny movie: [1,0,2,1,1,1,1]\n",
    "\t```\n",
    "    Train a logistic regression model to predict the sentiment. Compute the correlation between the predicted probabilities and the sentiment. Record the most positive and negative words.\n",
    "    Please note that in this model each word gets its sentiment parameter $S_w$ and the score for a sentence is \n",
    "\n",
    "$$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}S_w$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_vect(s):\n",
    "    v = np.zeros(K,dtype=np.int32)\n",
    "    for w in s.split():\n",
    "        i = Common_words.get(w,-1)\n",
    "        if i != -1:\n",
    "            v[i] += 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoded_DF = DF.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoded_DF['Phrase'] = list(map(sent_to_vect,DF['Phrase']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[3, 2, 0, 4, 1, 2, 0, 0, 1, 0, 0, 1, 0, 0, 2, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[1, 3, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>[1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>[1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Phrase  Sentiment\n",
       "0  [3, 2, 0, 4, 1, 2, 0, 0, 1, 0, 0, 1, 0, 0, 2, ...       0.25\n",
       "1  [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       1.00\n",
       "2  [0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...       0.25\n",
       "3  [1, 3, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0.75\n",
       "4  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0.25\n",
       "5  [1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...       1.00\n",
       "6  [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0.25\n",
       "7  [1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, ...       0.75\n",
       "8  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, ...       0.25\n",
       "9  [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, ...       0.25"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Encoded_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_train_df,c_test_df = train_test_split(Encoded_DF,train_size=7000,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression:\n",
    "    def __init__(self, max_iter=500, solver_calls=5,lambda_ = 0.1,Theta=None, solver=sopt.fmin_l_bfgs_b, debug=False):\n",
    "        self.Theta = Theta\n",
    "        self.solver_calls = solver_calls\n",
    "        self.max_iter = max_iter\n",
    "        self.solver = solver\n",
    "        self.debug = debug\n",
    "        self.lambda_ = lambda_\n",
    "    \n",
    "    def __sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))    \n",
    "    \n",
    "    def __logreg_loss(self, Theta, X, Y):\n",
    "        Theta = Theta.astype(np.float64)\n",
    "        X = X.astype(np.float64)\n",
    "        Y = Y.astype(np.float64)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Loss calculating... \",end=\"\")\n",
    "        Z = np.dot(Theta,X.T)\n",
    "        if self.debug:\n",
    "            print(f\" Z done... \",end=\"\")\n",
    "        SZ = self.__sigmoid(Z)\n",
    "        Y_ = Y[:,np.newaxis]\n",
    "        nll = -np.sum((Y_*np.log2(SZ+1e-50) + (1-Y_)*np.log2(1-SZ+1e-50)))\n",
    "        nll += (self.lambda_/2) * np.sum(Theta**2)\n",
    "        if self.debug:\n",
    "            print(f\" nll done... \",end=\"\")\n",
    "        grad = np.dot(X.T, (SZ - Y).T )\n",
    "        grad = grad.reshape(Theta.shape) + self.lambda_ * Theta\n",
    "        if self.debug:\n",
    "            print(f\" grad done... done \")\n",
    "        return nll, grad\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        Theta = self.Theta\n",
    "        if Theta is None:\n",
    "            Theta = np.ones(X.shape[1]+1)\n",
    "        \n",
    "        X_with_ones = np.hstack((np.ones((X.shape[0],1)),X))\n",
    "      \n",
    "        for i in tqdm(range(self.solver_calls), desc='Calculating Theta', position=0):\n",
    "            Theta = self.solver(lambda th: self.__logreg_loss(th, X_with_ones, y), \n",
    "                                Theta, maxiter=self.max_iter)[0]\n",
    "        self.Theta = Theta\n",
    "        \n",
    "    \n",
    "    def predict(self,X):\n",
    "        X_with_ones = np.hstack((np.ones((X.shape[0],1)),X))\n",
    "        preds = np.dot(self.Theta,X_with_ones.T)\n",
    "        return np.round(self.__sigmoid(preds) * 4) / 4\n",
    "    \n",
    "    def predict_ppb(self,X):\n",
    "        X_with_ones = np.hstack((np.ones((X.shape[0],1)),X))\n",
    "        preds = np.dot(self.Theta,X_with_ones.T)\n",
    "        return self.__sigmoid(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = Logistic_Regression(lambda_=0.5)\n",
    "X_train,y_train = np.stack(np.array(c_train_df['Phrase'])),np.array(c_train_df['Sentiment'])\n",
    "X_test,y_test = np.stack(np.array(c_test_df['Phrase'])),np.array(c_test_df['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Theta: 100%|█████████████████████████████████████████████████████████████████| 5/5 [03:00<00:00, 36.01s/it]\n"
     ]
    }
   ],
   "source": [
    "LR.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = LR.predict(X_test)\n",
    "preds_unrounded = LR.predict_ppb(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3642903858731197"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07987246566383258"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((preds - y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((preds_unrounded - y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Th = LR.Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stupid',\n",
       " 'worst',\n",
       " 'devoid',\n",
       " 'unpleasant',\n",
       " 'lacking',\n",
       " 'poor',\n",
       " 'horrible',\n",
       " 'terrible',\n",
       " 'incoherent',\n",
       " 'mess',\n",
       " 'depressing',\n",
       " 'barely',\n",
       " 'suffers',\n",
       " 'unfunny',\n",
       " 'lazy',\n",
       " 'poorly',\n",
       " 'inept',\n",
       " 'flat',\n",
       " 'mediocre',\n",
       " 'wasted']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_neg_arg = np.argsort(Th)[:20]\n",
    "most_neg_words = [Common_words_list[i-1] for i in most_neg_arg]\n",
    "most_neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feelgood',\n",
       " 'dazzling',\n",
       " 'masterpiece',\n",
       " 'intoxicating',\n",
       " 'wonderful',\n",
       " 'remarkable',\n",
       " 'refreshing',\n",
       " 'joyous',\n",
       " 'originality',\n",
       " 'imax',\n",
       " 'delightfully',\n",
       " 'follow',\n",
       " 'charmer',\n",
       " 'ahead',\n",
       " 'chilling',\n",
       " 'assured',\n",
       " 'amazing',\n",
       " 'pulls',\n",
       " 'mesmerizing',\n",
       " 'hilarious']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_pos_arg = np.argsort(Th)[::-1][:20]\n",
    "most_pos_words = [Common_words_list[i-1] for i in most_pos_arg]\n",
    "most_pos_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **[3p]** Now prepare an encoding in which negation flips the sign of the following words. For instance for our vocabulary the encodings become:\n",
    "    ```\n",
    "\t0: the\n",
    "\t1: good\n",
    "\t2: movie\n",
    "\t3: is\n",
    "\t4: not\n",
    "\t5: a\n",
    "\t6: funny\n",
    "    ```\n",
    "\t```\n",
    "\tgood:                           [0,1,0,0,0,0,0]\n",
    "\tnot good:                       [0,-1,0,0,1,0,0]\n",
    "\tnot not good:                   [0,1,0,0,0,0,0]\n",
    "\tthe movie is not a funny movie: [1,0,0,1,1,-1,-1]\n",
    "\t```\n",
    "\tFor best results, you will probably need to construct a list of negative words.\n",
    "\t\n",
    "\tAgain train a logistic regression classifier and compare the results to the Bag of Words approach.\n",
    "\t\n",
    "\tPlease note that this model still maintains a single parameter for each word, but now the sentence score is\n",
    "\n",
    "$$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}-1^{\\text{count of negations preceeding }w}S_w$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "negation_words = {'not','no','never','no','hardly','nobody','none','scarcely','nowhere','sparsely','scantly','seldom','sporadically','somewhat','infrequently','imperceptibly','rarely','comparatively','perceptibly','gradually','detectably'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_vect_with_neg(s):\n",
    "    v = np.zeros(K,dtype=np.int32)\n",
    "    neg = 1\n",
    "    for w in s.split():\n",
    "        i = Common_words.get(w,-1)\n",
    "        if i != -1:\n",
    "            v[i] += neg \n",
    "        if w in negation_words:\n",
    "            neg *= -1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[3, 0, 0, 0, -1, 2, 0, 0, 1, 0, 0, 1, 0, 0, 2,...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[1, 3, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>[1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>[1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Phrase  Sentiment\n",
       "0  [3, 0, 0, 0, -1, 2, 0, 0, 1, 0, 0, 1, 0, 0, 2,...       0.25\n",
       "1  [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       1.00\n",
       "2  [0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...       0.25\n",
       "3  [1, 3, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0.75\n",
       "4  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0.25\n",
       "5  [1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...       1.00\n",
       "6  [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0.25\n",
       "7  [1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, ...       0.75\n",
       "8  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, ...       0.25\n",
       "9  [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, ...       0.25"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Encoded_neg_DF = DF.copy()\n",
    "Encoded_neg_DF['Phrase'] = list(map(sent_to_vect_with_neg,DF['Phrase']))\n",
    "Encoded_neg_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_train_df,nc_test_df = train_test_split(Encoded_neg_DF,train_size=7000,shuffle=False)\n",
    "neg_X_train,y_train = np.stack(np.array(nc_train_df['Phrase'])),np.array(c_train_df['Sentiment'])\n",
    "neg_X_test,y_test = np.stack(np.array(nc_test_df['Phrase'])),np.array(c_test_df['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Theta: 100%|█████████████████████████████████████████████████████████████████| 5/5 [02:40<00:00, 32.09s/it]\n"
     ]
    }
   ],
   "source": [
    "LR_neg = Logistic_Regression(lambda_=0.5)\n",
    "LR_neg.fit(neg_X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_preds = LR_neg.predict(neg_X_test)\n",
    "neg_preds_unrounded = LR_neg.predict_ppb(neg_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35251798561151076"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(neg_preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08657619359058208"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((neg_preds - y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08132675077638069"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((neg_preds_unrounded - y_test)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **[5p]** Now also consider emphasizing words such as `very`. They can boost (multiply by a constant >1) the following words.\n",
    "\tImplement learning the modifying multiplier for negation and for emphasis. One way to do this is to introduce a model which has:\n",
    "\t- two modifiers, $N$ for negation and $E$ for emphasis\n",
    "\t- a sentiment score $S_w$ for each word \n",
    "And score each sentence as:\n",
    "$$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}N^{\\text{\\#negs prec. }w}E^{\\text{\\#emphs prec. }w}S_w$$\n",
    "\n",
    "You will need to implement a custom logistic regression model to support it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "emphance_words  = {'very', 'consistently', 'constantly', 'continually', 'inadvertently', 'mutually', 'simply', 'strongly', 'actively', 'energetically', 'firmly', 'fully', 'heartily', 'heavily', 'resolutely', 'robustly', 'solidly', 'staunchly', 'steadily', 'vigorously', 'completely', 'decidedly', 'forcibly', 'indomitably', 'invincibly', 'mightily', 'securely', 'stoutly', 'sturdily'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_vect_with_emph(s,N,E):\n",
    "    v = np.zeros(K,dtype=np.int32)\n",
    "    emph = 1\n",
    "    for w in s.split():\n",
    "        i = Common_words.get(w,-1)\n",
    "        if i != -1:\n",
    "            v[i] += emph\n",
    "        if w in negation_words:\n",
    "            emph *= -N\n",
    "        if w in emphance_words:\n",
    "            emph *= E\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[3, 0, 0, 0, -1, 2, 0, 0, 1, 0, 0, 1, 0, 0, 2,...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[1, 3, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>[1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>[1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Phrase  Sentiment\n",
       "0  [3, 0, 0, 0, -1, 2, 0, 0, 1, 0, 0, 1, 0, 0, 2,...       0.25\n",
       "1  [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       1.00\n",
       "2  [0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...       0.25\n",
       "3  [1, 3, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0.75\n",
       "4  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0.25\n",
       "5  [1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...       1.00\n",
       "6  [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0.25\n",
       "7  [1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, ...       0.75\n",
       "8  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, ...       0.25\n",
       "9  [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, ...       0.25"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Encoded_emph_DF = DF.copy()\n",
    "Encoded_emph_DF['Phrase'] = list(map(lambda s: sent_to_vect_with_emph(s,1,3),DF['Phrase']))\n",
    "Encoded_emph_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_train_df,ec_test_df = train_test_split(Encoded_emph_DF,train_size=7000,shuffle=False)\n",
    "emph_X_train,y_train = np.stack(np.array(ec_train_df['Phrase'])),np.array(c_train_df['Sentiment'])\n",
    "emph_X_test,y_test = np.stack(np.array(ec_test_df['Phrase'])),np.array(c_test_df['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Theta: 100%|█████████████████████████████████████████████████████████████████| 5/5 [02:54<00:00, 34.82s/it]\n"
     ]
    }
   ],
   "source": [
    "LR_emph = Logistic_Regression(lambda_=0.5)\n",
    "LR_emph.fit(emph_X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "emph_preds = LR_emph.predict(emph_X_test)\n",
    "emph_preds_unrounded = LR_emph.predict_ppb(emph_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33747547416612167"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(emph_preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08915140614780903"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((emph_preds - y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08301257948263047"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((emph_preds_unrounded - y_test)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **[2pb]** Propose, implement, and evaluate an extension to the above model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_vect_with_limited_emph(s,N,E):\n",
    "    v = np.zeros(K,dtype=np.int32)\n",
    "    emph = 1\n",
    "    for w in s.split():\n",
    "        i = Common_words.get(w,-1)\n",
    "        if i != -1:\n",
    "            v[i] += emph\n",
    "        if emph > 1:\n",
    "            emph -= 1\n",
    "        elif emph < 1:\n",
    "            emph += 1\n",
    "        if w in negation_words:\n",
    "            emph *= -N\n",
    "        if w in emphance_words:\n",
    "            emph *= E\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[3, 2, 0, 2, 1, 2, 0, 0, 1, 0, 0, 1, 0, 0, 2, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[1, 3, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>[1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>[1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Phrase  Sentiment\n",
       "0  [3, 2, 0, 2, 1, 2, 0, 0, 1, 0, 0, 1, 0, 0, 2, ...       0.25\n",
       "1  [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       1.00\n",
       "2  [0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...       0.25\n",
       "3  [1, 3, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0.75\n",
       "4  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0.25\n",
       "5  [1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...       1.00\n",
       "6  [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0.25\n",
       "7  [1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, ...       0.75\n",
       "8  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, ...       0.25\n",
       "9  [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, ...       0.25"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N,E = 1,3\n",
    "Encoded_lemph_DF = DF.copy()\n",
    "Encoded_lemph_DF['Phrase'] = list(map(lambda s: sent_to_vect_with_limited_emph(s,1,3),DF['Phrase']))\n",
    "Encoded_lemph_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lec_train_df,lec_test_df = train_test_split(Encoded_lemph_DF,train_size=7000,shuffle=False)\n",
    "lemph_X_train,y_train = np.stack(np.array(lec_train_df['Phrase'])),np.array(c_train_df['Sentiment'])\n",
    "lemph_X_test,y_test = np.stack(np.array(lec_test_df['Phrase'])),np.array(c_test_df['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Theta: 100%|█████████████████████████████████████████████████████████████████| 5/5 [03:10<00:00, 38.15s/it]\n"
     ]
    }
   ],
   "source": [
    "LR_lemph = Logistic_Regression(lambda_=0.5)\n",
    "LR_lemph.fit(lemph_X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemph_preds = LR_emph.predict(lemph_X_test)\n",
    "lemph_preds_unrounded = LR_emph.predict_ppb(lemph_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35251798561151076"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lemph_preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0822841726618705"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((lemph_preds - y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07722449889812918"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((lemph_preds_unrounded - y_test)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stem_(text):\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF['Phrase'] = list(map(lambda s: stem_(s),DF['Phrase']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df = train_test_split(DF,train_size=7000,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>a seri of escapad demonstr the adag that what ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>this quiet introspect and entertain independ i...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>even fan of ismail merchant s work i suspect w...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>a posit thrill combin of ethnographi and all t...</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>aggress selfglorif and a manipul whitewash</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6995</td>\n",
       "      <td>snoot will no doubt ralli to it caus trot out ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6996</td>\n",
       "      <td>it s better suit for the histori or biographi ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6997</td>\n",
       "      <td>buri an interest storylin</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6998</td>\n",
       "      <td>this one is a few bit funnier than mall s dud ...</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6999</td>\n",
       "      <td>the film has sever strong perform</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Phrase  Sentiment\n",
       "0     a seri of escapad demonstr the adag that what ...       0.25\n",
       "1     this quiet introspect and entertain independ i...       1.00\n",
       "2     even fan of ismail merchant s work i suspect w...       0.25\n",
       "3     a posit thrill combin of ethnographi and all t...       0.75\n",
       "4            aggress selfglorif and a manipul whitewash       0.25\n",
       "...                                                 ...        ...\n",
       "6995  snoot will no doubt ralli to it caus trot out ...       0.25\n",
       "6996  it s better suit for the histori or biographi ...       0.25\n",
       "6997                          buri an interest storylin       0.50\n",
       "6998  this one is a few bit funnier than mall s dud ...       0.75\n",
       "6999                  the film has sever strong perform       0.75\n",
       "\n",
       "[7000 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = defaultdict(int)\n",
    "for i in train_df.index:\n",
    "    sample = train_df.loc[i]\n",
    "    for word in sample['Phrase'].split():\n",
    "        WORDS[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10653"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2000\n",
    "Common_words_list = list(sorted(list(WORDS.keys()),key = WORDS.get,reverse=True))[:K]\n",
    "Common_words = {word:i for i,word in enumerate(Common_words_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_Encoded_DF = DF.copy()\n",
    "stemmed_Encoded_DF['Phrase'] = list(map(sent_to_vect,DF['Phrase']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_train_df,sc_test_df = train_test_split(stemmed_Encoded_DF,train_size=7000,shuffle=False)\n",
    "s_X_train,y_train = np.stack(np.array(sc_train_df['Phrase'])),np.array(c_train_df['Sentiment'])\n",
    "s_X_test,y_test = np.stack(np.array(sc_test_df['Phrase'])),np.array(c_test_df['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Theta: 100%|█████████████████████████████████████████████████████████████████| 5/5 [02:59<00:00, 35.81s/it]\n"
     ]
    }
   ],
   "source": [
    "LR_s = Logistic_Regression(lambda_=0.5)\n",
    "LR_s.fit(s_X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_preds = LR_emph.predict(s_X_test)\n",
    "s_preds_unrounded = LR_emph.predict_ppb(s_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22498364944408109"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(s_preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1510791366906475"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((s_preds - y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14379943024515346"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((s_preds_unrounded - y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
